# Лабораторная работа "Генеративные нейросети"

## Введение  
В данной работе исследовалась задача генерации текста на основе датасета субтитров фильмов. Основной целью было сравнение качества работы трех рекуррентных архитектур: SimpleRNN, однонаправленной LSTM и двунаправленной LSTM (BiLSTM) при посимвольной токенизации. Критериями оценки служили точность предсказания следующего символа (accuracy) и качество сгенерированного текста.

## Ход работы  

### Подготовка данных

Работа началась с загрузки датасета субтитров через Kaggle API и последующей предобработки текста. Предобработка включала приведение текста к нижнему регистру, удаление специальных символов и нормализацию пробелов. Затем была реализована посимвольная токенизация, в ходе которой создавался словарь символов (объемом около 100 элементов), формировались обучающие последовательности длиной 50 символов и определялись целевые символы для предсказания.

### Архитектуры моделей

Для исследования были разработаны три типа нейросетевых архитектур. Простая RNN-модель содержала embedding-слой размерностью 128, два RNN-слоя по 256 нейронов каждый, dropout-слои с коэффициентом 0.3-0.4 и полносвязные слои с ReLU/Softmax активациями. Эта архитектура, имеющая около 1 миллиона параметров, демонстрировала типичную для простых RNN проблему затухающих градиентов.

LSTM-модель сохранила аналогичную структуру, но использовала LSTM-слои вместо простых RNN. При том же количестве параметров (~1M) она обладала механизмом "ворот", что улучшало обработку долгосрочных зависимостей в тексте.

Наиболее сложной оказалась двунаправленная LSTM архитектура. В ней каждый LSTM-слой (256 нейронов) работал в двух направлениях, что суммарно давало 512 нейронов на слой. Эта модель, содержащая около 2 миллионов параметров, обеспечивала анализ контекста в обоих направлениях, но требовала наибольших вычислительных ресурсов.

### Процесс обучения

Все модели обучались с одинаковыми параметрами: размер батча 256, максимум 20 эпох обучения, оптимизатор Adam и функция потерь Sparse Categorical Crossentropy. Для улучшения процесса обучения применялись три ключевые стратегии: EarlyStopping (остановка при отсутствии улучшений точности на валидации в течение 7 эпох), ReduceLROnPlateau (автоматическое уменьшение learning rate при застревании) и ModelCheckpoint для сохранения лучших версий моделей. Данные были разделены на обучающую и валидационную выборки в соотношении 85/15.

## Результаты  

### Сравнение моделей
| Модель          | Ответственный       | Accuracy (val) | Loss (val) |
|----------------|---------------------|----------------|------------|
| SimpleRNN      | Илья Попов          | 0.5733         | 1.5698     |
| LSTM           | Александр Семин     | 0.5737         | 1.5636     |
| **BiLSTM**     | **Совместная работа** | **0.5839**     | **1.5303** |

###
Bidirectional LSTM показала лучшую точность и наименьший loss, превзойдя SimpleRNN и LSTM. Разница между LSTM и SimpleRNN оказалась незначительной (0.0004), что говорит о схожей эффективности этих архитектур на данном датасете. 

### Анализ генерации текста BiLSTM-модели

#### Влияние параметра температуры
Эксперименты с различными значениями температуры генерации показали интересные закономерности в работе модели. При низкой температуре (0.5) текст получается наиболее предсказуемым - модель склонна повторять уже знакомые шаблоны вроде "collection" или "'pose'", что делает вывод механическим, но при этом сохраняется относительная связность. 

Оптимальные результаты достигаются в диапазоне температур 0.8-1.0, где модель находит баланс между креативностью и осмысленностью. В этом режиме появляется заметное разнообразие в генерируемых последовательностях, при этом сохраняется общая структурная целостность текста.

Однако при повышении температуры до 1.2 модель становится излишне "креативной" - в тексте появляются странные артефакты и бессмысленные сочетания символов, что значительно ухудшает связность и читаемость результата. Это демонстрирует важность тщательного подбора параметра температуры для каждой конкретной задачи генерации текста.

#### Анализ по seed-фразам
| Тип фразы       | Особенности генерации                     |
|----------------|------------------------------------------|
| Эмоциональные (`i love`) | Более разнообразные продолжения          |
| Нейтральные (`hello`)    | Частые повторяющиеся паттерны            |
| Технические (`the movie`)| Появление артефактов       |

## Выводы

В ходе выполнения работы была проведена исследовательская работа по сравнению различных архитектур генеративных нейронных сетей для задачи генерации текста на основе датасета субтитров фильмов. На первом этапе был реализован полный пайплайн обработки данных, включающий загрузку датасета, предобработку текста (нормализацию, очистку от специальных символов) и посимвольную токенизацию с созданием обучающих последовательностей. Затем были последовательно реализованы и обучены три модели: простая RNN, однонаправленная LSTM и двунаправленная LSTM, каждая из которых содержала embedding-слой, рекуррентные слои и механизмы регуляризации (dropout).

Сравнительный анализ показал, что двунаправленная LSTM продемонстрировала наилучшие результаты с точностью 0.5839 на валидационной выборке, что превышает показатели других архитектур. При этом обычная LSTM и простая RNN показали схожие результаты (0.5737 и 0.5733 соответственно), что может свидетельствовать о недостаточной глубине архитектуры или особенностях датасета. Эксперименты с генерацией текста подтвердили, что BiLSTM лучше сохраняет контекст и производит более осмысленные продолжения. Полученные результаты наглядно демонстрируют преимущества двунаправленных архитектур для задач обработки последовательностей и подтверждают теоретические ожидания относительно их эффективности.